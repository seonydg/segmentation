{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unet - Chest CT Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kaggle datasets Download\n",
    "- 데이터 : Chest CT Segmentation(Chest CT scans together with segmentation masks for lung, heart, and trachea)\n",
    "- 캐글 데이터 주소: https://www.kaggle.com/datasets/polomarco/chest-ct-segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d polomarco/chest-ct-segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -qq '/content/chestct/chest-ct-segmentation.zip'\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv('/content/chestct/train.csv')\n",
    "data_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id(x):\n",
    "    return x.split('_')[0]\n",
    "\n",
    "data_raw['id'] = data_raw.ImageId.apply(lambda x: get_id(x))\n",
    "data_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cli_ids = data_raw.id.unique()\n",
    "len(cli_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(os.listdir('/content/chestct/images/images')), len(os.listdir('/content/chestct/masks/masks')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cli_id = -1\n",
    "cli_data = data_raw[data_raw.id == cli_ids[cli_id]]\n",
    "cli_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cli_data(data_raw, cli_id):\n",
    "    cli_ids = data_raw.id.unique()\n",
    "    cli_data = data_raw[data_raw.id == cli_ids[cli_id]]\n",
    "    \n",
    "    image_file = cli_data.imageId.values\n",
    "    mask_file = cli_data.Maskid.values\n",
    "    id_file = cli_data.id.values[0]\n",
    "\n",
    "    return id_file, image_file, mask_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "cli_id, image_files, mask_files = get_cli_data(data_raw, idx)\n",
    "canvas = np.zeros(shape=(512, 512*2+50, 3), dtype=np.uint8)\n",
    "\n",
    "for i in range(len(image_files)):\n",
    "    image = cv2.imread(os.path.join(data_dir, 'images', image_files[i]))\n",
    "    mask = cv2.imread(os.path.join(data_dir, 'images', mask_files[i]))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
    "    mask[mask < 240] = 0\n",
    "    mask[mask <= 240] = 255\n",
    "\n",
    "    canvas[:, :512, :] = image\n",
    "    canvas[:, 512:512*2+50, :] = mask\n",
    "\n",
    "    cv2.imshow('image', canvas)\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:\n",
    "        break\n",
    "    if key == ord('s'):\n",
    "        cv2.waitkey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 구축\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이미지와 마스크 파일이 하나로 묶여 있어, train, val, test 세트로 분리를 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 디렉토리 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir 'train' 'val' 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir 'train/images' 'train/masks' 'val/images' 'val/masks' 'test/images' 'test/masks'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'id'를 기준으로 분리- train, val, test 비율: 0.8, 0.1, 0.1\n",
    "- 'id'는 총 112개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "len(cli_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 나누기\n",
    "split_ratio = [0.8, 0.1, 0.1]\n",
    "train_len = int(len(cli_ids) * split_ratio[0])\n",
    "val_len = int(len(cli_ids) * split_ratio[1])\n",
    "test_len = len(cli_ids) - train_len - val_len\n",
    "\n",
    "print('{}, {}, {}'.format(train_len, val_len, test_len))\n",
    "\n",
    "train_ids = []\n",
    "val_ids = []\n",
    "test_ids = []\n",
    "\n",
    "for i in range(len(cli_ids)):\n",
    "    if 0 <= i < train_len:\n",
    "        train_ids.append(cli_ids[i])\n",
    "    elif train_len <= i < train_len + val_len:\n",
    "        val_ids.append(cli_ids[i])\n",
    "    elif train_len + val_len <= i:\n",
    "        test_ids.append(cli_ids[i])\n",
    "\n",
    "print('{}, {}, {}'.format(len(train_ids), len(val_ids), len(test_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 각각 이미지에서 나눈 'id'를 기준으로 각 디렉토리에 file 복사하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/content/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/image\n",
    "to_file_path = '/content/data/train/images/'\n",
    "from_file_path = '/content/chestct/images/images/'\n",
    "\n",
    "for file_name in os.listdir('/content/chestct/images/images'):\n",
    "    for id in train_ids:\n",
    "        if file_name.startswith(id):\n",
    "            shutil.copyfile(from_file_path + file_name, to_file_path + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train/mask\n",
    "to_file_path = '/content/data/train/masks/'\n",
    "from_file_path = '/content/chestct/masks/masks/'\n",
    "\n",
    "for file_name in os.listdir('/content/chestct/masks/masks'):\n",
    "    for id in train_ids:\n",
    "        if file_name.startswith(id):\n",
    "            shutil.copyfile(from_file_path + file_name, to_file_path + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val/image\n",
    "to_file_path = '/content/data/val/images/'\n",
    "from_file_path = '/content/chestct/images/images/'\n",
    "\n",
    "for file_name in os.listdir('/content/chestct/images/images'):\n",
    "    for id in val_ids:\n",
    "        if file_name.startswith(id):\n",
    "            shutil.copyfile(from_file_path + file_name, to_file_path + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val/mask\n",
    "to_file_path = '/content/data/val/masks/'\n",
    "from_file_path = '/content/chestct/masks/masks/'\n",
    "\n",
    "for file_name in os.listdir('/content/chestct/masks/masks'):\n",
    "    for id in val_ids:\n",
    "        if file_name.startswith(id):\n",
    "            shutil.copyfile(from_file_path + file_name, to_file_path + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test/image\n",
    "to_file_path = '/content/data/test/images/'\n",
    "from_file_path = '/content/chestct/images/images/'\n",
    "\n",
    "for file_name in os.listdir('/content/chestct/images/images'):\n",
    "    for id in test_ids:\n",
    "        if file_name.startswith(id):\n",
    "            shutil.copyfile(from_file_path + file_name, to_file_path + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test/mask\n",
    "to_file_path = '/content/data/test/masks/'\n",
    "from_file_path = '/content/chestct/masks/masks/'\n",
    "\n",
    "for file_name in os.listdir('/content/chestct/masks/masks'):\n",
    "    for id in test_ids:\n",
    "        if file_name.startswith(id):\n",
    "            shutil.copyfile(from_file_path + file_name, to_file_path + file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 잘 나누었는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세트로 잘 복사되었는지 확인\n",
    "print('train image:',len(os.listdir('/content/data/train/images')), '\\ttrain masks: ',len(os.listdir('/content/data/train/masks')))\n",
    "print('valid image:',len(os.listdir('/content/data/val/images')), '\\tvalid masks: ',len(os.listdir('/content/data/val/masks')))\n",
    "print('test  image:',len(os.listdir('/content/data/test/images')), '\\ttest  masks: ',len(os.listdir('/content/data/test/masks')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/content/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset():\n",
    "    def __init__(self, data_dir, phase, transform=None):\n",
    "        self.phase = phase\n",
    "        self.images_dir = os.path.join(data_dir, phase, 'images')\n",
    "        self.masks_dir = os.path.join(data_dir, phase, 'masks')\n",
    "        self.image_files = [file_name for file_name in os.listdir(self.image_dir) if file_name.endshith('jpg')]\n",
    "        self.mask_files = [file_name for file_name in os.listdir(self.masks_dir) if file_name.endshith('jpg')]\n",
    "        assert len(self.image_files) == len(self.mask_files)\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self,):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = cv2.imread(os.path.join(self.images_dir, self.image_files[index]))\n",
    "        image = cv2.resize(image, dsize=(224, 224), interpolation=cv2.INTER_LINEAR)\n",
    "        mask = cv2.imread(os.path.join(self.masks_dir, self.mask_files[index]))\n",
    "        mask = cv2.resize(mask, dsize=(224, 224), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        mask[mask < 240] = 0\n",
    "        mask[mask >= 240] = 255\n",
    "        mask = mask / 255.\n",
    "\n",
    "        mask_H, mask_W, mask_C = mask.shape\n",
    "        background = np.ones(shape=(mask_H, mask_W))\n",
    "        background[mask[..., 0] != 0] = 0\n",
    "        background[mask[..., 1] != 0] = 0\n",
    "        background[mask[..., 2] != 0] = 0\n",
    "\n",
    "        mask = np.concatenate([np.expand_dims(background, axis=-1), mask], axis=-1)\n",
    "        mask = np.argmax(mask, axis=-1, keepdims=False)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        target = torch.from_numpy(mask).long()\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    targets = []\n",
    "    for a, b in batch:\n",
    "        images.append(a)\n",
    "        targets.append(b)\n",
    "    images = torch.stack(images, dim=0)\n",
    "    targets = torch.stack(targets, dim=0)\n",
    "\n",
    "    return images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataloader(data_dir, batch_size=4):\n",
    "    dataloaders = []\n",
    "\n",
    "    train_dataset = MyDataset(data_dir=data_dir, phase='train', transform=transform)\n",
    "    dataloaders['train'] = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn) # shuffle = False\n",
    "\n",
    "    val_dataset = MyDataset(data_dir=data_dir, phase='val', transform=transform)\n",
    "    dataloaders['val'] = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = build_dataloader(data_dir=data_dir)\n",
    "\n",
    "for phase in ['train', 'val']:\n",
    "    for idx, data in enumerate(dataloaders[phase]):\n",
    "        images = data[0]\n",
    "        targets = data[1]\n",
    "        print(f'Image shape :  {images.shape}\\tmask shape :  {mask.shape}')\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16 Backbone 활용하여 UNET 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv_Layer(in_channels, out_channels, kernel_size=3, padding=1):\n",
    "    layers = nn.Sequential(\n",
    "                            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=padding),\n",
    "                            nn.BatchNorm2d(out_channels),\n",
    "                            nn.ReLU(inplace=True),\n",
    "\n",
    "                            nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=padding),\n",
    "                            nn.BatchNorm2d(out_channels),\n",
    "                            nn.ReLU(inplace=True)\n",
    "                            )\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UpConv_Layer(in_channels, out_channels):\n",
    "    layers = nn.Sequential(\n",
    "                            nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels, kernel_size=2, stride=2),\n",
    "                            nn.BatchNorm2d(out_channels),\n",
    "                            nn.ReLU(inplace=True)\n",
    "    )\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\baebi\\miniconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (32): ReLU(inplace=True)\n",
       "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (36): ReLU(inplace=True)\n",
       "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (39): ReLU(inplace=True)\n",
       "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (42): ReLU(inplace=True)\n",
       "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16 = models.vgg16_bn(weights=False)\n",
    "vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, weights):\n",
    "        super().__init__()\n",
    "\n",
    "        backbone = models.vgg16_bn(weights=weights).features\n",
    "        self.conv_block1 = nn.Sequential(*backbone[0:6])\n",
    "        self.conv_block2 = nn.Sequential(*backbone[6:13])\n",
    "        self.conv_block3 = nn.Sequential(*backbone[13:20])\n",
    "        self.conv_block4 = nn.Sequential(*backbone[20:27])\n",
    "        self.conv_block5 = nn.Sequential(*backbone[27:34], Conv_Layer(512, 1024, kernel_size=1, padding=0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encode_features = []\n",
    "        \n",
    "        out = self.conv_block1(x)\n",
    "        encode_features.append(out)\n",
    "\n",
    "        out = self.conv_block2(out)\n",
    "        encode_features.append(out)\n",
    "\n",
    "        out = self.conv_block3(out)\n",
    "        encode_features.append(out)\n",
    "\n",
    "        out = self.conv_block4(out)\n",
    "        encode_features.append(out)\n",
    "\n",
    "        out = self.conv_block5(out)\n",
    "\n",
    "        return out, encode_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(weights=False)\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "out, ft = encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 224, 224])\n",
      "torch.Size([1, 128, 112, 112])\n",
      "torch.Size([1, 256, 56, 56])\n",
      "torch.Size([1, 512, 28, 28])\n",
      "torch.Size([1, 1024, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "for i in ft:\n",
    "    print(i.shape)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.upconv_layer1 = UpConv_Layer(in_channels=1024, out_channels=512)\n",
    "        self.conv_block1 = Conv_Layer(in_channels=512*2, out_channels=512)\n",
    "\n",
    "        self.upconv_layer2 = UpConv_Layer(in_channels=512, out_channels=256)\n",
    "        self.conv_block2 = Conv_Layer(in_channels=256*2, out_channels=256)\n",
    "\n",
    "        self.upconv_layer3 = UpConv_Layer(in_channels=256, out_channels=128)\n",
    "        self.conv_block3 = Conv_Layer(in_channels=128*2, out_channels=128)\n",
    "        \n",
    "        self.upconv_layer4 = UpConv_Layer(in_channels=128, out_channels=64)\n",
    "        self.conv_block4 = Conv_Layer(in_channels=64*2, out_channels=64)\n",
    "\n",
    "    def forward(self, x, encode_features):\n",
    "        out = self.upconv_layer1(x)\n",
    "        out = torch.cat([out, encode_features[-1]], dim=1)\n",
    "        out = self.conv_block1(out)\n",
    "\n",
    "        out = self.upconv_layer2(out)\n",
    "        out = torch.cat([out, encode_features[-2]], dim=1)\n",
    "        out = self.conv_block2(out)\n",
    "\n",
    "        out = self.upconv_layer3(out)\n",
    "        out = torch.cat([out, encode_features[-3]], dim=1)\n",
    "        out = self.conv_block3(out)\n",
    "\n",
    "        out = self.upconv_layer4(out)\n",
    "        out = torch.cat([out, encode_features[-4]], dim=1)\n",
    "        out = self.conv_block4(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(weights=False)\n",
    "decoder = Decoder()\n",
    "x = torch.rand(1, 3, 224, 224)\n",
    "out, ftrs = encoder(x)\n",
    "out = decoder(out, ftrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 224, 224])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET(nn.Module):\n",
    "    def __init__(self, num_classes, weights):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(weights=weights)\n",
    "        self.decoder = Decoder()\n",
    "        self.head = nn.Conv2d(64, num_classes, kernel_size=1, padding=0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, encode_features = self.encoder(x)\n",
    "        out = self.decoder(out, encode_features)\n",
    "        out = self.head(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 224, 224])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = UNET(num_classes=4, weights=False)\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "out = model(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
